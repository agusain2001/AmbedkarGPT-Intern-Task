# ğŸ“ AmbedkarGPT â€“ AI Intern Assignment Project

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-Framework-green.svg)](https://langchain.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Welcome to **AmbedkarGPT**, a RAG (Retrieval-Augmented Generation) powered command-line Q&A system built as part of the **Kalpit Pvt Ltd AI Intern Hiring Assignment**. This project transforms a speech excerpt by **Dr. B. R. Ambedkar** from "Annihilation of Caste" into an interactive question-answering tool.

## ğŸ¯ What Does It Do?

You ask a question â†’ The system retrieves the most relevant text segments â†’ A local AI model (Mistral 7B) generates an answer **strictly from the retrieved context** â†’ No hallucinations, no external knowledge, just accurate retrieval-based responses.

---

## âš™ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| ğŸ Language | **Python 3.8+** | Core programming language |
| ğŸ”— Framework | **LangChain** | RAG pipeline orchestration |
| ğŸ¤– LLM | **Ollama (Mistral 7B)** | Local language model for answer generation |
| ğŸ§  Embeddings | **sentence-transformers/all-MiniLM-L6-v2** | Text-to-vector conversion |
| ğŸ—ƒï¸ Vector DB | **ChromaDB** | Local vector storage and similarity search |

**Key Features:**
- âœ… 100% Local - No API keys, no cloud dependencies, no costs
- âœ… Privacy-First - All data stays on your machine
- âœ… Zero Hallucination - Answers only from provided text
- âœ… Fast Retrieval - Vector similarity search in milliseconds

---

## ğŸ“‹ Prerequisites

Before you begin, ensure you have:

- **Python 3.8 or higher** installed
- **5 GB free disk space** (for Mistral model)
- **4 GB RAM minimum** (8 GB recommended)
- **Internet connection** (for initial setup only)

---

## ğŸš€ Installation & Setup

Follow these steps carefully to set up the project on your machine.

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/agusain2001/AmbedkarGPT-Intern-Task.git
cd AmbedkarGPT-Intern-Task
```

### 2ï¸âƒ£ Install Ollama & Pull Mistral Model

**Ollama** is required to run the Mistral 7B model locally.

#### ğŸ“¥ Install Ollama (Linux/macOS):
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### ğŸ“¥ Install Ollama (Windows):
Download from [ollama.ai](https://ollama.ai) and run the installer.

#### ğŸ“¦ Download the Mistral Model:
```bash
ollama pull mistral
```
*This downloads ~4 GB. First-time setup only.*

#### âœ… Verify Installation:
```bash
ollama run mistral "Hello"
```
You should see Mistral respond. Press `Ctrl+D` to exit.

**Important:** Keep Ollama running in the background before proceeding!

---

### 3ï¸âƒ£ Create a Virtual Environment

Creating a virtual environment isolates project dependencies.

```bash
# Create virtual environment
python3 -m venv venv

# Activate it (macOS/Linux)
source venv/bin/activate

# Activate it (Windows)
venv\Scripts\activate
```

You should see `(venv)` in your terminal prompt.

---

### 4ï¸âƒ£ Install Python Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**First-time note:** The sentence-transformers model (~80 MB) will download automatically on first run.

---

## â–¶ï¸ Running the Program

Once everything is installed and Ollama is running:

```bash
python main.py
```

### ğŸ¬ Expected Output:

```
======================================================================
               ğŸ“ AmbedkarGPT - Q&A System ğŸ“
======================================================================

A RAG-powered system to explore Dr. B.R. Ambedkar's speech on
the 'Annihilation of Caste' through interactive Q&A.

ğŸ” Checking prerequisites...
   âœ“ All prerequisites met

ğŸš€ Initializing RAG pipeline...

ğŸ“„ Loading speech.txt...
   âœ“ Loaded 1 document(s)

âœ‚ï¸  Splitting text into chunks...
   âœ“ Created 4 text chunks

ğŸ§  Creating embeddings with HuggingFace model...
   âœ“ Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2

ğŸ—ƒï¸  Storing embeddings in ChromaDB vector store...
   âœ“ Vector store created successfully

ğŸ” Setting up retriever...
   âœ“ Retriever configured (k=3 similarity search)

ğŸ¤– Connecting to Ollama with Mistral 7B...
   âœ“ Connected to Mistral model

â›“ï¸  Building RetrievalQA chain...
   âœ“ QA chain created successfully

======================================================================
                    âœ… SYSTEM READY!
======================================================================

ğŸ“– You can now ask questions based on the speech text.
ğŸ’¡ Examples:
   - What is the real remedy?
   - What is the problem with caste?
   - What does Ambedkar say about the shastras?

âŒ¨ï¸  Type 'quit', 'exit', or 'q' to stop.

â“ Ask a question: 
```

---

## ğŸ’¬ Example Usage

### Example 1: Understanding the Core Message
```
â“ Ask a question: What is the real remedy?

ğŸ’¬ Answer:
   The real remedy is to destroy the belief in the sanctity of the shastras.
```

### Example 2: Exploring the Problem
```
â“ Ask a question: What is the problem of caste according to Ambedkar?

ğŸ’¬ Answer:
   The problem of caste is not a problem of social reform. It is a problem of 
   overthrowing the authority of the shastras. Social reform alone cannot 
   eliminate caste as long as people believe in the sanctity of the scriptures.
```

### Example 3: Understanding the Analogy
```
â“ Ask a question: What analogy does he use for social reform?

ğŸ’¬ Answer:
   Dr. Ambedkar compares social reform to a gardener who constantly prunes 
   leaves and branches of a tree without ever attacking the roots. This 
   illustrates that superficial changes won't solve the core problem.
```

---

## ğŸ“‚ Project Structure

```
AmbedkarGPT-Intern-Task/
â”‚
â”œâ”€â”€ main.py              # Core application - RAG pipeline implementation
â”œâ”€â”€ speech.txt           # Source text - Dr. Ambedkar's speech excerpt
â”œâ”€â”€ requirements.txt     # Python dependencies with version constraints
â”œâ”€â”€ README.md            # This file - comprehensive documentation
â”œâ”€â”€ .gitignore          # Git ignore rules (excludes cache, venv, etc.)
â”‚
â””â”€â”€ chroma/             # (Auto-generated) ChromaDB vector store data
```

---

## ğŸ”§ Troubleshooting

### Issue: "Error connecting to Ollama"
**Solution:**
1. Check if Ollama is running: `ollama list`
2. Restart Ollama service
3. Verify Mistral is installed: `ollama pull mistral`

### Issue: "speech.txt not found"
**Solution:**
Ensure you're running `python main.py` from the project root directory where `speech.txt` is located.

### Issue: "ImportError" or missing modules
**Solution:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Issue: Slow first-time startup
**Expected behavior:** The first run downloads the sentence-transformers model (~80 MB). Subsequent runs are fast.

### Issue: Out of memory
**Solution:** 
- Close other applications
- Mistral 7B requires ~4 GB RAM
- Consider using a smaller model or upgrading RAM

---

## ğŸ§ª How It Works (Technical Deep-Dive)

### The RAG Pipeline Architecture:

```
User Question
     â†“
[1] Text Embedding (Query â†’ Vector)
     â†“
[2] Similarity Search (Find relevant chunks)
     â†“
[3] Context Retrieval (Top 3 chunks)
     â†“
[4] LLM Generation (Mistral processes context + query)
     â†“
Answer (Based only on retrieved context)
```

### Step-by-Step Breakdown:

1. **Document Loading**: `speech.txt` is loaded and preprocessed
2. **Text Chunking**: Split into 250-character chunks with 50-char overlap for context continuity
3. **Embedding Generation**: Each chunk is converted to a 384-dimensional vector using sentence-transformers
4. **Vector Storage**: Embeddings stored in ChromaDB for fast similarity search
5. **Query Processing**: User question â†’ embedded â†’ similarity search â†’ top 3 chunks retrieved
6. **Answer Generation**: Mistral 7B receives chunks + question â†’ generates contextual answer

### Why This Approach?

- **No Hallucination**: LLM only sees retrieved text, can't make things up
- **Explainable**: You can trace answers back to specific text segments
- **Efficient**: Only relevant context sent to LLM, reducing tokens and latency
- **Scalable**: Works with documents of any size (just add more chunks)

---

## ğŸ“ Learning Outcomes

This project demonstrates understanding of:

âœ… **RAG Architecture** - Retrieval-Augmented Generation fundamentals  
âœ… **Vector Embeddings** - Converting text to semantic vectors  
âœ… **Similarity Search** - Finding relevant information via cosine similarity  
âœ… **LLM Integration** - Combining retrieval with language models  
âœ… **Local AI Stack** - Building AI apps without cloud dependencies  
âœ… **Python Best Practices** - Clean code, error handling, documentation  

---

## ğŸš€ Possible Enhancements

Want to take this further? Here are some ideas:

- ğŸ¨ **Better UI**: Add colorized terminal output with `rich` or `colorama`
- ğŸ“Š **Source Citations**: Show which text chunks were used for each answer
- ğŸ” **Advanced Retrieval**: Implement hybrid search (keyword + semantic)
- ğŸ’¾ **Persistent Storage**: Keep vector store across sessions
- ğŸ“ˆ **Performance Metrics**: Track retrieval accuracy and response time
- ğŸŒ **Web Interface**: Build a Streamlit or Flask frontend
- ğŸ“š **Multi-Document**: Extend to handle multiple speeches/books
- ğŸ¤– **Model Swapping**: Easy switching between different LLMs

---

## ğŸ“ Assignment Compliance Checklist

This project meets all requirements:

- âœ… Python 3.8+ with clean, commented code
- âœ… LangChain framework for RAG orchestration
- âœ… ChromaDB as local vector store
- âœ… HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)
- âœ… Ollama with Mistral 7B (free, local, no API keys)
- âœ… All 5 pipeline steps implemented correctly
- âœ… Public GitHub repository with proper structure
- âœ… requirements.txt with all dependencies
- âœ… Comprehensive README.md documentation
- âœ… speech.txt included in repository

---

---

## ğŸ™ Acknowledgments

- **Dr. B. R. Ambedkar** - For his profound writings on social justice
- **LangChain Community** - For excellent RAG framework and documentation
- **Ollama Team** - For making local LLMs accessible
- **HuggingFace** - For open-source embedding models
- **Kalpit Pvt Ltd** - For this learning opportunity

---

## ğŸ“ Support

If you encounter any issues:

1. Check the [Troubleshooting](#-troubleshooting) section above
2. Ensure all prerequisites are installed correctly
3. Verify Ollama is running: `ollama list`
4. Check Python version: `python --version` (needs 3.8+)


---

**Built with â¤ï¸ using 100% local, open-source AI tools**
